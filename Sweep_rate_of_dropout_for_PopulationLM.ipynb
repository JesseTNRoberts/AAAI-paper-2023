{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FRLxg2wN4fpbm61S8cZoM2_giexiKP-7",
      "authorship_tag": "ABX9TyPQ0uBMA3ydejZDUFfavKdg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JesseTNRoberts/AAAI-paper-2023/blob/main/Sweep_rate_of_dropout_for_PopulationLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/JesseTNRoberts/PopulationLM\n",
        "!pip install git+https://github.com/JesseTNRoberts/minicons_modded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DTNFI4DPGFBD",
        "outputId": "496ba5b9-096a-4265-88bf-3965d8f94b9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/JesseTNRoberts/PopulationLM\n",
            "  Cloning https://github.com/JesseTNRoberts/PopulationLM to /tmp/pip-req-build-q1oa7hrm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/JesseTNRoberts/PopulationLM /tmp/pip-req-build-q1oa7hrm\n",
            "  Resolved https://github.com/JesseTNRoberts/PopulationLM to commit 328a6628146ff22e2e0eb8261007b0e9b7af0d0c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: PopulationLM\n",
            "  Building wheel for PopulationLM (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PopulationLM: filename=PopulationLM-0.1-py3-none-any.whl size=3526 sha256=3e22158395ffe35df64ea224ce98d872889762c08b897f92d5bb954fd0a9be82\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-aqdm9bvy/wheels/76/d3/b7/994b47d195d749b9752de5e275517c30ab4ef097e508ec8f2f\n",
            "Successfully built PopulationLM\n",
            "Installing collected packages: PopulationLM\n",
            "Successfully installed PopulationLM-0.1\n",
            "Collecting git+https://github.com/JesseTNRoberts/minicons_modded\n",
            "  Cloning https://github.com/JesseTNRoberts/minicons_modded to /tmp/pip-req-build-pylnxoxw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/JesseTNRoberts/minicons_modded /tmp/pip-req-build-pylnxoxw\n",
            "  Resolved https://github.com/JesseTNRoberts/minicons_modded to commit 6ed7ef122fc12453554a725c1027c6d7b166cf93\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from minicons==0.2.17) (1.5.3)\n",
            "Collecting torch<2.0.0,>=1.8.0 (from minicons==0.2.17)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.4.1 (from minicons==0.2.17)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<2.0.0,>=1.26.7 in /usr/local/lib/python3.10/dist-packages (from minicons==0.2.17) (1.26.16)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.3.5->minicons==0.2.17) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.3.5->minicons==0.2.17) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.3.5->minicons==0.2.17) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.0.0,>=1.8.0->minicons==0.2.17) (4.7.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.0.0,>=1.8.0->minicons==0.2.17)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.0.0,>=1.8.0->minicons==0.2.17)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.0.0,>=1.8.0->minicons==0.2.17)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.0.0,>=1.8.0->minicons==0.2.17)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.8.0->minicons==0.2.17) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.8.0->minicons==0.2.17) (0.41.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.4.1->minicons==0.2.17) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers<5.0.0,>=4.4.1->minicons==0.2.17)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.4.1->minicons==0.2.17) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.4.1->minicons==0.2.17) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.4.1->minicons==0.2.17) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.4.1->minicons==0.2.17) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.4.1->minicons==0.2.17)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.4.1->minicons==0.2.17)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.4.1->minicons==0.2.17) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<5.0.0,>=4.4.1->minicons==0.2.17) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.3.5->minicons==0.2.17) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.4.1->minicons==0.2.17) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.4.1->minicons==0.2.17) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.4.1->minicons==0.2.17) (3.4)\n",
            "Building wheels for collected packages: minicons\n",
            "  Building wheel for minicons (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minicons: filename=minicons-0.2.17-py3-none-any.whl size=24742 sha256=d5246249737b4746e51435cd1ce6742d34f9b8708e009765e3186a688446053a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dv_rl68_/wheels/15/0e/ec/7fb33880467af25612dba7aec20864fb5523bca9247bf3e09b\n",
            "Successfully built minicons\n",
            "Installing collected packages: tokenizers, safetensors, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, huggingface-hub, transformers, torch, minicons\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.16.4 minicons-0.2.17 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 safetensors-0.3.1 tokenizers-0.13.3 torch-1.13.1 transformers-4.31.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyAYuTIs9sBz",
        "outputId": "61102ecd-4520-4b56-f1b9-c5125379fcb0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "565\n",
            "565\n",
            "toy.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [29:47<00:00, 35.76s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "565\n",
            "565\n",
            "toy.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [30:59<00:00, 37.20s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "565\n",
            "565\n",
            "toy.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [29:54<00:00, 35.89s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "565\n",
            "565\n",
            "toy.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [30:33<00:00, 36.68s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "565\n",
            "565\n",
            "toy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [29:39<00:00, 35.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "565\n",
            "565\n",
            "toy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [30:33<00:00, 36.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "565\n",
            "565\n",
            "toy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [29:39<00:00, 35.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "565\n",
            "565\n",
            "toy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [29:03<00:00, 34.86s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import argparse\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from minicons import scorer\n",
        "import PopulationLM as pop\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "committee_size = 50\n",
        "\n",
        "def shuffle_sentence(sentence, word):\n",
        "    '''\n",
        "        returns the shuffled form of a sentence while preserving the\n",
        "        multi-word expression order for the focus word.\n",
        "    '''\n",
        "    sentence = sentence.replace(\".\", \"\")\n",
        "    if len(word.split()) > 1:\n",
        "        sentence = sentence.replace(word, \"@\".join(word.split())).split()\n",
        "    else:\n",
        "        sentence = sentence.split()\n",
        "    random.shuffle(sentence)\n",
        "\n",
        "    return \" \".join(sentence).replace(\"@\", \" \").capitalize() + \".\"\n",
        "\n",
        "inpath = '/content/drive/MyDrive/Data/rosch1975_alternate.csv'\n",
        "model_name = 'gpt2'\n",
        "batch_size = 565\n",
        "device = 'cpu'\n",
        "lm_type = 'incremental'\n",
        "\n",
        "# make results dir: ../data/typicality/results/(dataset)/model_name.csv\n",
        "components = inpath.split(\"/\")\n",
        "data_dir = \"/\".join(components[0:-1])\n",
        "dataset_name = components[-1].split(\".\")[0]\n",
        "results_dir = f\"{data_dir}/results/{dataset_name}_popLM_50\"\n",
        "\n",
        "dataset = []\n",
        "with open(inpath, \"r\") as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    column_names = reader.fieldnames\n",
        "    for row in reader:\n",
        "        dataset.append(list(row.values()))\n",
        "\n",
        "if lm_type == \"masked\" or lm_type == \"mlm\":\n",
        "    transformer = scorer.MaskedLMScorer(model_name, device)\n",
        "elif lm_type == \"incremental\" or lm_type == \"causal\":\n",
        "    transformer = scorer.IncrementalLMScorer(model_name, device)\n",
        "\n",
        "if \"/\" in model_name:\n",
        "    model_name = model_name.replace(\"/\", \"_\")\n",
        "\n",
        "num_params = [sum(p.numel() for p in transformer.model.parameters())] * len(dataset)\n",
        "\n",
        "stimuli_loader = DataLoader(dataset, batch_size = batch_size, num_workers=0)\n",
        "\n",
        "for val in np.arange(0.1, 0.9, 0.1):\n",
        "  # convert the internal model to use MC Dropout\n",
        "  pop.DropoutUtils.convert_dropouts(transformer.model)\n",
        "  pop.DropoutUtils.activate_mc_dropout(transformer.model, activate=True, random=val)\n",
        "\n",
        "  results = []\n",
        "  control_results = []\n",
        "  conclusion_only = []\n",
        "\n",
        "  # create a lambda function alias for the method that performs classifications\n",
        "  call_me = lambda prefixes, queries: transformer.conditional_score(prefixes, queries, reduction=lambda x: (x.sum(0).item(), x.mean(0).item(), x.tolist()))\n",
        "\n",
        "  for batch in stimuli_loader:\n",
        "      premise = list(batch[0])\n",
        "      conclusion = list(batch[1])\n",
        "\n",
        "      population = pop.generate_dropout_population(transformer.model, lambda: call_me(premise, conclusion), committee_size=committee_size)\n",
        "\n",
        "      print(len(premise))\n",
        "      print(len(conclusion))\n",
        "\n",
        "      print(conclusion[0])\n",
        "      # create the population identities\n",
        "\n",
        "      outs = [item for item in tqdm(pop.call_function_with_population(transformer.model, population, lambda: call_me(premise, conclusion)),\n",
        "                                    total=committee_size)]\n",
        "      transposed_outs = [[row[i] for row in outs] for i in range(len(outs[0]))]\n",
        "      priming_scores = [score for score in transposed_outs]\n",
        "      results.extend(priming_scores)\n",
        "\n",
        "  data_out = list(zip(*dataset))\n",
        "  new_col_names = column_names\n",
        "\n",
        "  data_out.append(results)\n",
        "  new_col_names += [\"score (sum, mean, [list)\"]\n",
        "\n",
        "  data_out.append(num_params)\n",
        "  data_out.append([model_name] * len(results))\n",
        "  new_col_names += [\"params\", \"model\"]\n",
        "\n",
        "  with open(results_dir + f\"/{model_name}_dropout_\"+str(val)+\".csv\", \"w\") as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(new_col_names)\n",
        "      writer.writerows(list(zip(*data_out)))\n"
      ]
    }
  ]
}